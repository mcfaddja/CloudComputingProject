\section{Introduction}

Initially, we were going to do a software/systems comparison, with the software/systems chosen for comparison in this project being two different NoSQL database system.  These systems would have been deployed/run/operated in several different ways, including \emph{SaaS}\footnote{\textbf{SaaS :} Software as a service.} implementations, \emph{containerized} implementations, and \emph{native installations}.  The goal of the project would have been to understand the performance characteristics of each deployment method \emph{\textbf{and}} to quantify the costs of each deployment method, with the costs being calculated based on the hourly cost to operate, the initial time \& costs required for setup, and the maintenance requirement of a deployment.  Additionally, performance of the systems and deployments would have been measured using the time required to carry out various database operations, under a set of several different conditions, as well as the CPU, memory, and network loads imposed by the various deployments under the same set of conditions. As we proceeded with the project, however, we discovered that there was no way to properly cover all of these topics within a single project 


%--------------------------------

\subsection{Changed Questions}

Ultimately, we decided that our initial approach was too complex and beyond the intended scope of the project; therefore, were limited ourselves to comparing DynamoDB from AWS and BigTable from Google's Cloud Services.  We also changed our comparison of the services to be based on metrics measuring their performance, features, usability, and cost; while also setting a specific and narrow scope for each metric.  The performance metric is limited to separately measuring both data throughput and query latency under a variety of load a replication conditions; while the features metric will concentrate on features available from each provider, such as provisioning and tools for interacting with their respective service.  Our usability metric will focus on both the ease of implementing software to use each service, as well as the availability in terms of the number of supported languages, the number of available APIs \& SDKs, and the availability of third-party software for interacting with each service.  Finally, our cost metric will focus on the costs associated with the setup and operation of each service, however, it will also attempt to cover costs which might be encountered when implementing software making use of each service.


%--------------------------------

\subsection{Service Change}

Unfortunately, BigTable is difficult to deploy for even a simple test case, as it requires the configuration of multiple cloud services on Google's cloud and the installation of multiple Linux-only software packages, namely rBase \& Hadoop.  Thus, we were again forced to alter our plan, this time by swapping BigTable from Google for CosmosDB from Azure.  This swap also lead to an additional feature to consider and condition to test under, the replication two or more of each cloud service's availability zones.




%--------------------------------
%--------------------------------


\section{Research Questions}

We decided on our metrics of performance, features, usability, and cost based on five research questions.  Each question was designed based on one or two metrics choices from our first three metrics (performance, features, and usability), with cost being considered separately over all relevant questions.  The research questions we developed are 

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\Large{\textbf{\arabic*}):}]
	\item How do the costs and provisioning implementations compare?
	\vspace{0.05in}
	\item Are we getting what we pay for (\emph{provision}) W.R.T.\footnote{W.R.T. = with respect to} capacity and space?
	\vspace{0.05in}
	\item How efficient is use of the service W.R.T. latency and throughput?
	\vspace{0.05in}
	\item How easy is it to implement connections in software or applications? How easy is it to push/pull data?  What kind of tools are available from each provider for interacting with the database?
	\vspace{0.05in}	\item What kinds of replication are available and how are they accomplished?
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Provisioning Costs and Implementations}

When answering, "\emph{\textbf{How do the costs and provisioning implementations compare?}}" we are interested in exploring two things.  The first thing in which we are interested is each service's implementation of database provisioning, with specific attention being paid to how provisioning is characterized by each service.  Specifically, we what to know how each service parameterizes the provisioning of databases in their NoSQL services.  To cover this part of our first research question, we have the following "sub-questions" :

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\large{\textbf{1 - \Alph*}):}]
	\item Are reads and writes provisioned separately or together?  
	\vspace{0.05in}
	\item What counts as a database "\emph{item}"?
	\vspace{0.05in}
	\item Is there an assumption with regard to the size of database "\emph{items}" or is that an additional provisioning parameter?
	\vspace{0.05in}
	\item Is the disk space required to store the database used as a parameter?
\end{enumerate}
\end{spacing}

The second and last thing we wish to explore is the cost model used by each provider, especially how it relates to database use and data size.  To cover the second part of this research question, we will add the following "sub-questions" :

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\large{\textbf{1 - \Alph*}):}]
	\addtocounter{enumi}{4} 
	\item Are reads and writes charged at the same or different rates?
	\vspace{0.05in}
	\item Does the provisioned cost of database depend on its level of use?
	\vspace{0.05in}
	\item Does the size of database "items" affect the cost of provisioning?
	\vspace{0.05in}
	\item Is the disk space required to store the database included in the previous provisioning costs or does it constitute another, additional cost dimension?
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Getting what we pay for}

Given that using a NoSQL database on a cloud service incurs an hourly cost that can be significant, it is natural to wonder, "\emph{\textbf{Are we getting what we pay for (}provision\textbf{) W.R.T. capacity and space?}}".  To answer this question, we will test each service using two different methods of testing.  In the first testing method, we will push a large dataset up to each database while measuring the throughput in \textbf{items written per second}.  Similarly, in the second method, we will pull a large dataset down from each database while again measuring the throughput in \textbf{items written per second}.  


%--------------------------------

\subsection{Efficiency of throughput and latency}

No matter the type of database, the primary metric of any heavily used database is latency of queries, measured in \textbf{ms}.\footnote{\textbf{ms} = milliseconds}  This led us to ask, "\emph{\textbf{How efficient is use of the service W.R.T. latency and throughput?}}" as one of our research questions.  To answer this question, we will run queries on the database under three different conditions with each condition representing a different level of load on the database service.  These load levels are

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\large{\textbf{3 - \Alph*}):}]
	\item \textbf{\underline{None} :} No load on the database other than the query being measured.
	\vspace{0.05in}
	\item \textbf{\underline{Half-load} :} Pushing or pulling data to or from the database at half of the provisioned write or read capacity.
	\vspace{0.05in}
	\item \textbf{\underline{Full-load} :} Pushing or pulling data to or from the database at the full provisioned write or read capacity.
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Ease of use}

The goal of having any type of database, regardless of database type or location, is to efficiently store data for retrieval by means of some software or application.  Furthermore, in most cases, a database must be populated with data before it can be used, therefore there must be a way of initially populating the database after its creation. These considerations led us to ask, "\emph{\textbf{How easy is it to implement connections in software or applications?}}" and, "\emph{\textbf{How easy is it to push/pull data?}}" which we combined into our fourth research question.  To answer these questions we considered the general areas of database APIs/Models, software SDKs/APIs, and tools provided by each service.

When looking at the Database APIs/Models area, we wanted to look at the database systems or data storage/retrieval models offered by each service.  Specifically, we wanted to know how many different database implementations/languages each service supported or provided.  As both services were NoSQL databases, we also wanted to see if either service offered any unique or interesting data storage/retrieval models.  In-line with looking at different database languages, we wanted to look at what programing languages or SDKs could be used for writing software to connect to each database service.  Importantly, we wanted to know how well developed the  libraries and APIs were in each supported language and what, if any, costs would be incurred in order to use a particular supported language.  Additionally, since the software or application connected to that database could also be provided by a third-party, we wanted to see what third-party software was available for each service.  

Finally, we wanted to see what tools each cloud provider had available for interacting with their NoSQL database services, as well as compare the tool or tools offered by each service.  When comparing the tools, we wanted to know if there were any costs associated with using them and if those costs were one-time (\emph{buy the tool}) or if the costs would recur in some way.  The second, and last, aspect of exploring the provided tools was a comparison of each tool's ease of use.


%--------------------------------

\subsection{Replication}

The final area we were interested in covered database replication, as replication of a database improves its reliability and can improve its performance.  This lead us to ask, "\emph{\textbf{What kinds of replication are available and how are they accomplished?}}" as our fifth and final research question.  Finally, this question also led us to our first discovery.  We found that \textbf{\underline{AWS's DynamoDB}} does not offer replication as a single database between different AWS availability zones, while \textbf{\underline{Azure's CosmosDB}} does offer it between Azure's different availability zones.  As we will see when we describe the experiments we ran, this also led to us attempting to run experiments on \textbf{\underline{Azure's CosmosDB}} with and without replication between different Azure availability zones.




%--------------------------------
%--------------------------------


\section{Experiments}

The experiments we ran can be classified according to four distinct criteria, all based on one or more of our research questions.  These four criterial are as follows :

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\large{\textbf{\arabic*}):}]
	\item The subject of the experiment, where the subject is which service is being tested and if replication is being used, based on the availability of replication from the service in question.
	\vspace{0.05in}
	\item The primary metric of the experiment, with available primary metrics being either total data throughput\footnote{total data throughput = total number of database entries/items being pushed up to or pulled down from the database} and query latency.
	\vspace{0.05in}
	\item The throughput type being used or measured in the experiment, with the possible types of throughput being pushing data up to the database or pulling data down from the database.
	\vspace{0.05in}
	\item The load conditions under which the experiment was conducted, where the load condition is characterized by the level\footnote{level = measured as a fraction or percentage of the total provisioned read and/or write capacity.} and type\footnote{type of load = pushing data up, pulling data down, or some combination.} of load.
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Subjects}

We had three different classifications for the \emph{\textbf{subject}} of an experiment with one from AWS and two from Azure.  These experimental subject classifications are as follows : 

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\Large{\textbf{\Alph*}):}]
	\item AWS's DynamoDB
	\vspace{0.05in}
	\item Azure's CosmosDB \textbf{\underline{\emph{without}}} replication across availability zones.
	\vspace{0.05in}
	\item Azure's CosmosDB \textbf{\underline{\emph{with}}} replication across availability zones.
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Primary Metrics}

There were two primary metrics used in this project, with each experiment only measuring a single primary metric.  The primary metrics we used are :

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\Large{\textbf{\Alph*}):}]
	\item Data Throughput, measured in terms of the number of database entries/items being read and/or written per second (\textbf{items / sec} or \textbf{R/U}).
	\vspace{0.05in}
	\item Query Latency, which we measured in terms of the time (\textbf{milliseconds} or \textbf{ms}) required to return the result of a simple query\footnote{simple query = no joins, sorts, transforms, etc. just submit an index value and return the corresponding entry.}.
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Throughput Types}

We considered just two types of Data Throughput and we only considered them separately\footnote{"\emph{considered them separately} = i.e. one or the other, not both}.  These types of Data Throughput are as follows :

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\Large{\textbf{\Alph*}):}]
	\item Pushing, where we pushed data up to the database.
	\vspace{0.05in}
	\item Pulling, where were pull data down from the database.
\end{enumerate}
\end{spacing}


%--------------------------------

\subsection{Load Conditions}

When running an experiment, we would use one of five distinct kinds load conditions, where each load condition is characterized in terms of two different parameters.  The parameters used to characterize each load condition were the level of load\footnote{level of load = as a fraction or percentage of the total provisioned I/O capacity} and the direction of the load.  The types of load conditions we use are as follows :

\vspace{0.1in}
\begin{spacing}{1.2}
\begin{enumerate}[label=\large{\textbf{\Alph*}):}]
	\item \textbf{\underline{None} :} No load on the database other than the query being measured.
	\vspace{0.05in}
	\item \textbf{\underline{Half-load pushing} :} Pushing data to the database at half of the provisioned write capacity.
	\vspace{0.05in}
	\item \textbf{\underline{Half-load pulling} :} Pulling data to the database at half of the provisioned write capacity.
	\vspace{0.05in}
	\item \textbf{\underline{Full-load pushing} :} Pushing data to the database at the full provisioned write capacity.
	\vspace{0.05in}
	\item \textbf{\underline{Full-load pulling} :} Pulling data to the database at the full provisioned write capacity.
\end{enumerate}
\end{spacing}




%--------------------------------
%--------------------------------


\section{Results}










































































